---

name: calculate-times
run-name: calculate-times-${{ inputs.year }}-${{ inputs.mode }}

on:
  workflow_dispatch:
    inputs:
      # Input values must match those in params.yaml
      mode:
        required: true
        description: Mode of travel
        default: 'auto'
        type: choice
        options:
          - car
          - bicycle
          - foot

      year:
        required: true
        description: Census/OSM data year
        default: '2020'
        type: choice
        options:
          - '2020'
          - '2021'
          - '2022'
          - '2023'
          - '2024'

      override_geographies:
        required: false
        description: |
          Comma-separated geographies to limit run e.g. county,tract.
          Will run all if null
        type: string

      override_states:
        required: false
        description: |
          Comma-separated state codes to run e.g. 01,06.
          Will run all if null
        type: string

env:
  AWS_DEFAULT_REGION: us-east-1
  # See: https://github.com/aws/aws-cli/issues/5262#issuecomment-705832151
  AWS_EC2_METADATA_DISABLED: true
  PYTHONUNBUFFERED: "1"

jobs:
  setup-jobs:
    runs-on: ubuntu-24.04

    outputs:
      states: ${{ steps.create-state-jobs.outputs.param }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Create state jobs
        id: create-state-jobs
        uses: ./.github/actions/parse-gh-input
        with:
          param_path: '.input.state'
          param_override: '${{ inputs.override_states }}'

  run-job:
    runs-on: ubuntu-24.04
    needs: setup-jobs
    strategy:
      # Don't fail all chunks if one fails
      fail-fast: false
      matrix:
        state: ${{ fromJSON(needs.setup-jobs.outputs.states) }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Cloudflare credentials
        uses: ./.github/actions/setup-cloudflare-s3
        with:
          CLOUDFLARE_S3_API_ACCESS_KEY_ID: ${{ secrets.CLOUDFLARE_S3_API_ACCESS_KEY_ID }}
          CLOUDFLARE_S3_API_SECRET_ACCESS_KEY: ${{ secrets.CLOUDFLARE_S3_API_SECRET_ACCESS_KEY }}

      - name: Install DVC
        uses: ./.github/actions/setup-dvc

      - name: Fetch locations data
        uses: ./.github/actions/fetch-locations

      - name: Fetch network data
        shell: bash
        working-directory: 'data'
        run: |
          path=mode=${{ inputs.mode }}/year=${{ inputs.year }}/geography=state/state=${{ matrix.state }}
          aws s3 cp --quiet --endpoint-url \
            https://${{ vars.CLOUDFLARE_ACCOUNT_ID }}.r2.cloudflarestorage.com \
             s3://opentimes-resources/osrmnetwork/${path}/osrmnetwork.tar.zst \
             ./intermediate/osrmnetwork/${path}/osrmnetwork.tar.zst \
            --profile cloudflare
          tar -xf ./intermediate/osrmnetwork/"$path"/osrmnetwork.tar.zst .

      - name: Install uv
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true
          cache-suffix: "site-data"
          cache-dependency-glob: |
            pyproject.toml
            uv.lock

      - name: Install Python dependencies
        id: install-python-dependencies
        shell: bash
        run: |
          uv python install
          uv venv
          uv pip install ".[site,data]"

      - name: Increase swap space
        uses: ./.github/actions/increase-swap

      - name: Create jobs per geography
        id: create-geo-jobs
        uses: ./.github/actions/parse-gh-input
        with:
          param_path: '.input.census.geography.all'
          param_override: '${{ inputs.override_geographies }}'

      - name: Run job(s)
        shell: bash
        working-directory: 'data'
        run: |
          # Start the Docker backend before running jobs
          docker run -d -p 5333:5000 -v "./build:/data" \
            osrm/osrm-backend osrm-routed --algorithm ch \
            --max-table-size 100000000 /data/${{ matrix.state }}.osrm

          geographies='${{ steps.create-geo-jobs.outputs.param }}'
          geographies_array=($(echo "$geographies" | jq -r '.[]'))
          for geo in "${geographies_array[@]}"; do
            echo "Starting job with parameters: mode=${{ inputs.mode }}, year=${{ inputs.year }}, geography=${geo}, state=${{ matrix.state }}"
            uv run ./src/calculate_times.py \
              --mode ${{ inputs.mode }} --year ${{ inputs.year }} \
              --geography "$geo" --state ${{ matrix.state }} \
              --centroid-type weighted \
              --write-to-s3
          done
