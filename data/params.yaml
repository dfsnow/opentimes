---

# S3 bucket and prefix to write output data to. Uses Cloudflare so requires
# a custom S3 endpoint
s3:
  profile: 'cloudflare'
  data_bucket: 'opentimes-data'
  public_bucket: 'opentimes-public'
  public_data_url: 'https://data.opentimes.org'
  endpoint_url: 'https://fcb279b22cfe4c98f903ad8f9e7ccbb2.r2.cloudflarestorage.com'
  account_id: 'fcb279b22cfe4c98f903ad8f9e7ccbb2'

# Parameters used to control the chunking of work on GitHub Actions
actions:
  # The max number of jobs to create for a workflow. GitHub allows up to 256
  n_chunks: 256

  # The minimum number of origins to include in a job. Higher = fewer jobs
  # that take longer. Lower = more jobs that finish quicker
  min_chunk_size: 400

times:
  # Travel times output version. Follows SemVer (kind of):
  # 1. MAJOR version increments for incompatible API changes
  # 2. MINOR version increments when new data or fields are added
  # 3. PATCH version increments for backwards-compatible data updates
  version: '0.0.1'

  # Valhalla costing models to calculate times for, see:
  # https://valhalla.github.io/valhalla/api/turn-by-turn/api-reference/#costing-models
  mode:
    - auto
    - bicycle
    - pedestrian
    #- multimodal

  # Maximum number of OD pairs to process in a single matrix::CostMatrix call.
  # Necessary because larger matrices will cause it to choke
  max_pairs: 100

  # Coordinates are snapped to the OSM street network before time calculation.
  # Setting this to true will use the snapped coordinates directly in the
  # matrix API calls
  use_snapped: true

input:
  # Distance in meters to buffer each state boundary by when clipping the
  # national road network. Should be slightly higher than `destination_buffer_m`
  # so that all points can be reached via the clipped network
  network_buffer_m: 340000

  # Distance in meters to buffer each state boundary when searching for
  # destination points to route to. Larger buffer = more points = longer
  # overall compute time
  destination_buffer_m: 300000

  # Years for which to collect input data. Each year will grab the
  # corresponding TIGER/Line Census files, the Jan 1 Geofabrik North America
  # OSM extract, and all appropriate GTFS feeds
  year:
    - '2020'
    - '2021'
    - '2022'
    - '2023'
    - '2024'

  # States are treated as the "unit of work" in the pipeline. Each state gets
  # origin/destination points, a GeoJSON buffer, and an OSM extract/network
  state:
    - '01'
    - '02'
    - '04'
    - '05'
    - '06'
    - '08'
    - '09'
    - '10'
    - '11'
    - '12'
    - '13'
    - '15'
    - '16'
    - '17'
    - '18'
    - '19'
    - '20'
    - '21'
    - '22'
    - '23'
    - '24'
    - '25'
    - '26'
    - '27'
    - '28'
    - '29'
    - '30'
    - '31'
    - '32'
    - '33'
    - '34'
    - '35'
    - '36'
    - '37'
    - '38'
    - '39'
    - '40'
    - '41'
    - '42'
    - '44'
    - '45'
    - '46'
    - '47'
    - '48'
    - '49'
    - '50'
    - '51'
    - '53'
    - '54'
    - '55'
    - '56'

  # Census geographies to create travel times. All routing is performed between
  # geographic units of the same type, never between geography (i.e. tract to
  # tract)
  census:
    geography:
      all:
        - state
        - county
        - county_subdivision
        - tract
        - block_group
        - zcta
      national:
        - state
        - county
        - zcta
      by_state:
        - county_subdivision
        - tract
        - block_group
        - block  # only used to fetch TIGER files (not used for routing)

output:
  # Compression type and level to use for final output Parquet files
  compression:
    type: zstd
    level: 12
