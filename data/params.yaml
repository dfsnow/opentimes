---

# S3 bucket and prefix to write output data to. Uses Cloudflare so requires
# a custom S3 endpoint
s3:
  profile: 'cloudflare'
  data_bucket: 'opentimes-data'
  public_bucket: 'opentimes-public'
  public_data_url: 'https://data.opentimes.org'
  endpoint_url: 'https://fcb279b22cfe4c98f903ad8f9e7ccbb2.r2.cloudflarestorage.com'
  account_id: 'fcb279b22cfe4c98f903ad8f9e7ccbb2'

times:
  # Travel times output version. Follows SemVer (kind of):
  # 1. MAJOR version increments for incompatible API changes
  # 2. MINOR version increments when new data or fields are added
  # 3. PATCH version increments for backwards-compatible data updates
  version: '0.0.1'

  # OSRM profiles to calculate times for using the default settings of each one
  # https://github.com/Project-OSRM/osrm-backend/blob/master/docs/profiles.md
  mode:
    - car
    - bicycle
    - foot

  # Maximum size of chunk of origins AND destinations to process in a single
  # call to OSRM. Useful for limiting total OSRM memory consumption
  max_split_size: 5000

  # Coordinates are snapped to the OSM street network before time calculation
  use_snapped: true

  # If OSRM fails on the first pass, the time calculator begins a recursive
  # binary search to try to "go around" and origin-destination pairs causing
  # the failure. The full depth search can take a long time; this parameter
  # trades off search time and completeness
  max_recursion_depth: 5

input:
  # Distance in meters to buffer each state boundary by when clipping the
  # national road network. Should be slightly higher than `destination_buffer_m`
  # so that all points can be reached via the clipped network
  network_buffer_m: 340000

  # Distance in meters to buffer each state boundary when searching for
  # destination points to route to. Larger buffer = more points = longer
  # overall compute time
  destination_buffer_m: 300000

  # Years for which to collect input data. Each year will grab the
  # corresponding TIGER/Line Census files, the Jan 1 Geofabrik North America
  # OSM extract, and all appropriate GTFS feeds
  year:
    - '2020'
    - '2021'
    - '2022'
    - '2023'
    - '2024'

  # States are treated as the "unit of work" in the pipeline. Each state gets
  # origin/destination points, a GeoJSON buffer, and an OSM extract/network
  state:
    - '01'
    - '02'
    - '04'
    - '05'
    - '06'
    - '08'
    - '09'
    - '10'
    - '11'
    - '12'
    - '13'
    - '15'
    - '16'
    - '17'
    - '18'
    - '19'
    - '20'
    - '21'
    - '22'
    - '23'
    - '24'
    - '25'
    - '26'
    - '27'
    - '28'
    - '29'
    - '30'
    - '31'
    - '32'
    - '33'
    - '34'
    - '35'
    - '36'
    - '37'
    - '38'
    - '39'
    - '40'
    - '41'
    - '42'
    - '44'
    - '45'
    - '46'
    - '47'
    - '48'
    - '49'
    - '50'
    - '51'
    - '53'
    - '54'
    - '55'
    - '56'

  # Census geographies to create travel times. All routing is performed between
  # geographic units of the same type, never between geography (i.e. tract to
  # tract)
  census:
    geography:
      all:
        - state
        - county
        - county_subdivision
        - tract
        - block_group
        - zcta
      national:
        - state
        - county
        - zcta
      by_state:
        - county_subdivision
        - tract
        - block_group  # not currently used, too time-intensive
        - block  # only used to fetch TIGER files (not used for routing)

output:
  # Compression type and level to use for final output Parquet files
  compression:
    type: zstd
    level: 12
